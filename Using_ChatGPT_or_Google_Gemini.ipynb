{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/rizabukhari/Agentic-AI-projects/blob/main/Using_ChatGPT_or_Google_Gemini.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fb6rdwlCsCGt"
      },
      "source": [
        "# Using ChatGPT or Google Gemini with Python for real-world tasks using thier APIs"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {
        "id": "2evPp14fy258"
      },
      "outputs": [],
      "source": [
        "!pip install openai --quiet"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5N_Lly09l0P9"
      },
      "source": [
        "## Optional: Install Google Gemini\n",
        "\n",
        "Google Gemini API is free (till now). You can get a key [here](https://aistudio.google.com/app/apikey), just need to sign in with your google account. Gemini may not be available fully in EU."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {
        "id": "UKbTnx8pl8kt",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3a54e575-11f7-4495-d879-2d87f7d1637b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[?25l     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/53.1 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m53.1/53.1 kB\u001b[0m \u001b[31m3.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h\u001b[?25l   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/719.4 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m719.4/719.4 kB\u001b[0m \u001b[31m30.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h"
          ]
        }
      ],
      "source": [
        "# !pip install -q -U google-generativeai\n",
        "!pip install -q -U google-genai"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CiwGjVWK4q6F"
      },
      "source": [
        "## Load OpenAI API Credentials\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "metadata": {
        "id": "ryheOZuXxa41"
      },
      "outputs": [],
      "source": [
        "from google.colab import userdata\n",
        "import os\n",
        "openai_key = userdata.get('OPENAI_API_KEY')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "metadata": {
        "id": "kDe44J0N0NcC"
      },
      "outputs": [],
      "source": [
        "import openai\n",
        "openai.api_key = openai_key"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LS7koM2emZ_M"
      },
      "source": [
        "## Load Gemini API credentials\n",
        "\n",
        "Run this section only if you are using Google Gemini"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {
        "id": "nxJAcO1MmhRo"
      },
      "outputs": [],
      "source": [
        "import google.generativeai as genai\n",
        "GOOGLE_API_KEY = userdata.get('GEMINI_API_KEY')\n",
        "genai.configure(api_key=GOOGLE_API_KEY)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### OR"
      ],
      "metadata": {
        "id": "I3Zn1JsdM4dQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# The client gets the API key from the environment variable `GEMINI_API_KEY`.\n",
        "import os\n",
        "from google import genai\n",
        "os.environ[\"GEMINI_API_KEY\"] = userdata.get('GEMINI_API_KEY')\n",
        "\n",
        "client = genai.Client()"
      ],
      "metadata": {
        "id": "2NaVRkFIM0Mf"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "7eHl0fP9gzf6"
      },
      "outputs": [],
      "source": [
        "os.environ[\"OPENAI_API_KEY\"] = userdata.get('OPENAI_API_KEY')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "aivThNd-gzf7"
      },
      "outputs": [],
      "source": [
        "from IPython.display import HTML"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VDWhgxCy5bA6"
      },
      "source": [
        "## Create ChatGPT and Google Gemini Chat Completion Access Function\n",
        "\n",
        "This function will use the [Chat Completion API](https://platform.openai.com/docs/api-reference/chat/create) to access ChatGPT for us and return responses\n",
        "\n",
        "We also add the capability to access the [Google Gemini API](https://ai.google.dev/tutorials/python_quickstart) if you want to use Gemini Pro instead of ChatGPT"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from openai import OpenAI\n",
        "client = OpenAI()\n",
        "\n",
        "response = client.responses.create(\n",
        "    model=\"gpt-5-nano\",\n",
        "    input=\"Write a one-sentence bedtime story about a unicorn.\"\n",
        ")\n",
        "\n",
        "print(response.output_text)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HgQXyIvKz0nv",
        "outputId": "743f03f3-a80c-4670-bc01-04e3439777eb"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Under a velvet night, a gentle unicorn trotted beside a quiet lake, curled up on a bed of moss, and drifted off to sleep while the stars hummed him a lullaby.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "id": "kA9gVCwK0WKd"
      },
      "outputs": [],
      "source": [
        "def get_completion(prompt, model=\"gpt\"):\n",
        "  if model == \"gpt\":\n",
        "    messages = [{\"role\": \"user\", \"content\": prompt}]\n",
        "    response = openai.chat.completions.create(\n",
        "        model=\"gpt-5\", # This model name might need adjustment (e.g., gpt-3.5-turbo)\n",
        "        messages=messages,\n",
        "        temperature=0.1, # degree of randomness of the model's output\n",
        "    )\n",
        "    return response.choices[0].message.content\n",
        "\n",
        "  # if model == \"gemini\":\n",
        "  #   model = genai.GenerativeModel('gemini-2.5-flash-lite') # \"gemini-1.5-flash-latest\": # 'gemini-pro':\n",
        "  #   response = model.generate_content(prompt)\n",
        "  #   return response.text\n",
        "\n",
        "  if model == \"gemini\":\n",
        "    # Assuming 'client' is the genai.Client() initialized in cell 2NaVRkFIM0Mf.\n",
        "    response = client.models.generate_content(\n",
        "    model=\"gemini-2.5-flash\", contents=prompt\n",
        "    )\n",
        "    return response.text\n",
        "\n",
        "  else:\n",
        "    return \"LLM not configured! Please configure logic for specific model in get_completion()\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "id": "3qyCGTzkgzf_"
      },
      "outputs": [],
      "source": [
        "# This code block is commented out as it caused a 'return outside function' error.\n",
        "# The logic for 'gemini-1.5-flash-latest' should be integrated into the 'get_completion' function\n",
        "# if a different Gemini model needs to be supported beyond 'gemini-2.5-flash'.\n",
        "#\n",
        "# if model == 'gemini-1.5-flash-latest':\n",
        "#     model = genai.GenerativeModel('gemini-1.5-flash-latest')\n",
        "#     response = model.generate_content(prompt)\n",
        "#     return response.text"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def get_completion(prompt, model=\"gpt\"):\n",
        "  if model == \"gpt\":\n",
        "    client = OpenAI()\n",
        "    response = client.responses.create(\n",
        "        model=\"gpt-5-nano\",\n",
        "        input=prompt\n",
        "    )\n",
        "    return response.output_text\n",
        "\n",
        "  elif model == \"gemini\": # Changed to elif for proper branching\n",
        "    client = genai.Client()\n",
        "    response = client.models.generate_content(\n",
        "    model=\"gemini-2.5-flash\", contents=prompt\n",
        "    )\n",
        "    return response.text\n",
        "\n",
        "  else:\n",
        "    return \"LLM not configured! Please configure logic for specific model in get_completion()\""
      ],
      "metadata": {
        "id": "awvQ-ztCyzdp"
      },
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from openai import OpenAI\n",
        "from google import genai\n",
        "\n",
        "def get_completion(prompt, model=\"gpt\"):\n",
        "    if model == \"gpt\":\n",
        "        client = OpenAI()\n",
        "        response = client.responses.create(\n",
        "            model=\"gpt-5-nano\",\n",
        "            input=prompt\n",
        "        )\n",
        "        return response.output_text\n",
        "\n",
        "    elif model == \"gemini\":\n",
        "        client = genai.Client()\n",
        "        response = client.models.generate_content(\n",
        "            model=\"gemini-2.5-flash\",\n",
        "            contents=prompt\n",
        "        )\n",
        "        return response.text\n",
        "\n",
        "    else:\n",
        "        return \"LLM not configured! Use model='gpt' or model='gemini'.\"\n"
      ],
      "metadata": {
        "id": "znPHxqr62Fku"
      },
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(get_completion(\"Say hello in 3 words.\", model=\"gpt\"))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gxB0k4oW3znN",
        "outputId": "4d4fa028-a3bd-4924-b6d2-d3bd4b7464de"
      },
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Hello there, friend.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "r6L1tETs06Go"
      },
      "source": [
        "## Exercise-1: Text Generation with both ChatGPT and Google Gemini\n",
        "- Get ChatGPT to generate text by asking it to write a story\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 34,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "R0VDZwEG1B8N",
        "outputId": "5d6e56b8-2568-477f-b70f-147d7991df2a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "In the edge of Bristlewood, where pines whispered and the brook tinkled like tiny silver coins, lived a chipmunk named Niko. His fur wore the color of toasted chestnuts, and his eyes danced with quick, curious light. Niko believed that winter’s bite could be softened only by a snug pantry, and his pockets—two shallow hollows under his cheeks—were always full of acorns, chestnuts, and little treasures he found along the way.\n",
            "\n",
            "Every morning, Niko would scurry along a path that smelled of rain and pine needles, lining his stash in a neat little row inside a hollow log that stood like a friendly guard at the edge of his home. He could tell by the weight of each item how much winter would demand, and he liked the order of it all—the way the acorns nestled and settled, the way the breeze could not steal a single one without his knowing.\n",
            "\n",
            "One autumn afternoon, as the sun wore a gold coat and the leaves sighed red and brown, a sudden gust knocked more than just leaves from the trees. A dozen acorns rolled down toward the brook’s edge and into the water, and Niko’s careful rows jostled and toppled. He chased after them, ears flattening against his head, and watched the current carry his hoard away like small, stubborn boats.\n",
            "\n",
            "On the other side of the brook stood a willow with a lean, droopy smile, where a little mouse named Mero lived. Mero was more patient than bold, and his fur held the quiet brown of earth after rain. He had traveled farther than most mice dreamed, always carrying a tiny notebook where he sketched good hiding places and safe shortcuts through Bristlewood.\n",
            "\n",
            "Seeing Niko’s frantic scramble, Mero called softly, “Ho, friend, you’ve met your river. It asks for trust as much as for luck.”\n",
            "\n",
            "Niko frowned. “Trust? I trust myself to gather, not to drift.” He watched a few more acorns skip onto the water’s surface and drift away.\n",
            "\n",
            "“Well, if winter’s a river too, you’ll need more than speed to cross it,” Mero said, tapping the notebook with a tiny paw. “You’ll need friends who know the currents. I know a dozen safe caches where trees have taught them to hide their hearts.”\n",
            "\n",
            "Niko hesitated, the word “friends” tasting new and strange on his tongue. He had always done things alone—paced steps, counted nut heaps, kept everyone at the distance of a curious glance.\n",
            "\n",
            "That night, a soft rain began, and Bristlewood hummed with the first breath of winter’s approach. Niko sat on a fallen log, hooves to the rain, thinking of his lost rows. A shadow detached itself from the maple trunk; it was Kestrel, a young crow who had learned to shield her nest by listening to the forest’s stories. She landed lightly on the log and clicked her beak, as if tasting the air for danger or laughter.\n",
            "\n",
            "“Looking for shelter or something else?” Kestrel asked.\n",
            "\n",
            "“Both,” Niko admitted, surprised at how the word sounded in his mouth. “I’ve lost part of my stash, and I’m not sure I’ll find it again.”\n",
            "\n",
            "Kestrel blinked, then nodded toward the small valley beyond Bristlewood where a line of stones formed a rough circle. “There’s a place there where the forest keeps many things safe—food, stories, a memory or two. You’ll see if you listen.”\n",
            "\n",
            "The next morning, with rain patted down like a drum in the leaves, Niko found Mero again, showing him the notebook’s pages filled with tiny maps and careful notes: a chestnut hollow behind the old cedar, a crevice beneath the mossy log by the brook, a hollow in a yew where seeds lay like coins in a pocket. They spoke softly, agreeing to work together, to share caches not as a loan but as a small, simple trust.\n",
            "\n",
            "So began the making of what they called the Nut Bank. Mero mapped hidden hollows and taught Niko how to tuck acorns in a way that even if the wind blew hard, the stash would not slide away. Kestrel used her keen eyes to watch the far trees and warn them of trouble. Other forest creatures joined: a patient hedgehog who could press a pebble in place to mark a spot, a dormouse who counted the days until frost and reminded everyone when to add more, a jay who could crack a stubborn shell without breaking the nut inside.\n",
            "\n",
            "When winter finally arrived, Bristlewood wore a quiet, glittering coat of snow. The Nut Bank looked like a village of tiny cupboards, each log and hollow labeled with signs painted in moss and lichen. The animals could rely on each other now, not because they were stronger alone, but because they had learned to be stronger together.\n",
            "\n",
            "Niko would wake to hear Mero humming a slow tune as he brushed pine needles from a stash and whispered, “Here lies a year’s dream, tucked in a safe corner.” He would listen to Kestrel’s call from the treetops, a bright note that reminded them all to keep an eye out for one another. And sometimes, when a heavy gust would threaten to sweep away more than nuts, the forest would stand still for a breath, and then shift with a new sense of trust.\n",
            "\n",
            "Winter passed slowly, and spring came with a soft, patient green. The Nut Bank remained, not as a monument to one clever chipmunk, but as a chorus of many voices. They’d learned that a pile of nuts can be a good thing, but a shared life is a better shelter. Niko still gathered fast and smart, but now he did so with his friends in mind, knowing that the best hoard is not a bowl of acorns in a log, but a circle of neighbors who will share the warmth when the snow arrives.\n",
            "\n",
            "And so Bristlewood slept and woke, content in the knowledge that, this year at least, winter would be fed by more than one clever chipmunk. It would be fed by a village of small, brave creatures who chose to keep each other safe. Niko closed his eyes one night and felt the soft, sure pulse of the Nut Bank beneath the ground, and for the first time, he understood that home wasn’t just where his stash waited—it was where his friends stood beside him, ready for whatever came next.\n"
          ]
        }
      ],
      "source": [
        "response = get_completion(\"Write a short story about a chipmunk.\")\n",
        "print(response)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 35,
      "metadata": {
        "id": "jQRe9cLPgzgB",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "cd37d4b5-1816-4126-f36d-5ca5c2b09d8e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "5834\n"
          ]
        }
      ],
      "source": [
        "print(len(response))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "065uztCC8m1R",
        "outputId": "110e236e-f7bf-432e-81c7-1631f9841b13"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "None\n"
          ]
        }
      ],
      "source": [
        "# with Gemini\n",
        "response = get_completion(\"Write a short story about a college student.\", model='gemini')\n",
        "print(response)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kfnG0qe3gzgC"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GEVszGl8gzgD"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1TFZjzuGjCOw"
      },
      "source": [
        "## Exercise-2: Let's try out Zero Shot Prompting!\n",
        "\n",
        "- Let's get the model to answer the question about Generative AI\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 36,
      "metadata": {
        "id": "aBm9ef4MgzgD"
      },
      "outputs": [],
      "source": [
        "from IPython.display import display, Markdown"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 37,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 154
        },
        "id": "KK-kjmMoi5rO",
        "outputId": "1a734275-e3aa-4866-b14d-e43be9020fbb"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "- Generative AI refers to systems that learn from large datasets to model how data is distributed and then generate new, original content (text, images, audio, code) that resembles the training data.\n\n- It uses models like large language models, diffusion models, and GANs; enables tasks such as writing, design, and content creation, but can produce incorrect or biased outputs (hallucinations) and raises safety, copyright, and misuse concerns."
          },
          "metadata": {}
        }
      ],
      "source": [
        "response = get_completion(prompt='Explain Generative AI in 2 bullet points')\n",
        "display(Markdown(response))"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "response = get_completion(prompt='Explain AI Agents in 5 bullet points')\n",
        "display(Markdown(response))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 227
        },
        "id": "l6se9pA9Lf_F",
        "outputId": "f8d1afea-6c64-4dc4-fde3-c53d865329c3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "- **Definition and Purpose**: AI agents are autonomous entities that perceive their environment through sensors and act upon that environment using actuators. Their primary purpose is to achieve specific goals or tasks by making decisions based on their perceptions and pre-defined objectives.\n\n- **Types of AI Agents**: There are several types of AI agents, including simple reflex agents, model-based reflex agents, goal-based agents, utility-based agents, and learning agents. Each type varies in complexity and capability, from basic rule-following to sophisticated decision-making and learning.\n\n- **Perception and Action**: AI agents operate in a loop of perceiving their environment, processing the information, and taking actions. This cycle allows them to interact with and adapt to dynamic environments, making them suitable for tasks ranging from simple automation to complex problem-solving.\n\n- **Autonomy and Adaptability**: AI agents are designed to operate with a degree of autonomy, meaning they can make decisions without human intervention. They can also adapt to changes in their environment, improving their performance over time through learning and experience.\n\n- **Applications**: AI agents are used in various fields, including robotics, virtual assistants, autonomous vehicles, gaming, and financial services. Their ability to process large amounts of data and make informed decisions makes them valuable in enhancing efficiency and effectiveness across different domains."
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 145
        },
        "id": "VmnwGrskn_oz",
        "outputId": "58d39fe2-97e5-4a24-9bc0-8ccac6e5ed40"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "Here's an explanation of Generative AI in two bullet points:\n\n*   **Generative AI creates new, original content.** Instead of just analyzing or classifying existing data, it learns patterns and structures from vast datasets to produce novel text, images, music, code, or other forms of media.\n\n*   **It's driven by probabilistic models and deep learning.** These models, such as transformers and diffusion models, predict the most likely next elements in a sequence (like words in a sentence or pixels in an image) based on what they've learned from training data, allowing them to generate coherent and contextually relevant outputs."
          },
          "metadata": {}
        }
      ],
      "source": [
        "# with Gemini\n",
        "response = get_completion(prompt='Explain Generative AI in 2 bullet points',\n",
        "                          model='gemini')\n",
        "display(Markdown(response))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6NDLIaTFgzgF"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VGx9M87ggzgF"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aLHqAhm1gzgG"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8MD1AgaUDZ_c"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.12"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}